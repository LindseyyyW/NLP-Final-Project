{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7770550,
          "sourceType": "datasetVersion",
          "datasetId": 4545841
        },
        {
          "sourceId": 7771713,
          "sourceType": "datasetVersion",
          "datasetId": 4546613
        },
        {
          "sourceId": 7795580,
          "sourceType": "datasetVersion",
          "datasetId": 4563851
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'openbkqa:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4545841%2F7770550%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240316%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240316T060343Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4c6c93274cd637a40fabb24454a075289e1df8bb830f3b0ac88fa092b940ccaf4610044e3ae955fd836d179b9592e209e41561980dba250d92664bbdbc9cd7467ab5376060c9bdfe3527e6ca0787c2ba25ae2bd23021d89f9dfd6f9b679b269db31452df432b083330f301e50a47e3dcfb439776c518af746c386376c4974be793671efa5cc2dc22195ee6f051b412774bddbc99bdba3f683284eda567853386afcda127f22cc2d1b9fbcc767400dc2dd6df2eab72c5c3969e154ec4e519d438ebfe94bdc95cf39d673935db1b60e1ca2761a495a4c15c5f5be4df64e5f5f62e53992c807e93ff46c8373abbc50cd92e96e95663e99fcc17ea5812269084e9a8,openbook:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4546613%2F7771713%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240316%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240316T060343Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6aed470d4fea6f3bf5eb0b617efed7d3a518d8b34a04dd5aad17171d5ecdc12764256ac4c4760ba6ca9643ce8a36a593555248d2ef96b465372194c2b57b81f338c60b73e18be61a48b5f5c2062ecbede6e1671ecec2b2eaecbaa0080d1a9a8900fe16895057ff187e0d0a53526cb0e67bcde340ec6e2fa7ef47dfc986f7baaf8cff70f78d8b5990d78a042d13a4d1ceeb6a989a3ae0d7c739fb1de607e556d2253e57e2f3b4b18ed5adeda0a6c03ed23fed4ced9ebb02ad6966fde7daee984373780df9cb1600992ccb84c40efb58b8943bad53f56d74502b195af1f1febb999bbae2d6d4a81a5454f2e8e03e8c06923ec31386766926ce5baa2114935b17b6,genmccheckpoints:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4563851%2F7795580%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240316%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240316T060344Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc54a0fecccc659311571a4977df052c1cacad1e6fbe679da092e265eb5cfaeadf9b7a4903034aef02a2f78bd7aa90931425e0c35b18611aad509c037623d3899a16c22cad723f4a5ade3a3f53c8fca96011bcfef759b92e811a9c19b8c7c30a84f6de84cfc3687fd16fda18acb9a633964833532d0b2b67eb0bd1f0365f7af985feb404449d3496cc0e62544da14130c375d5a76b713e77ad8840f3ef0d36cdecdc7daa64dfe421a540b108b42fdadfc6d836aabfa74dd05ae953195a4fc261599c967549607f65dc33f5ede5182c2f6ffc6391acf0c6fdd2a7b5af1eeda4f306c93336bf2106b9b6422cc08b5f03f7a94cbb4aff4d768f346311f65a7f7b60b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "69q9kO712_Io"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:26.933692Z",
          "iopub.execute_input": "2024-03-08T19:30:26.934625Z",
          "iopub.status.idle": "2024-03-08T19:30:43.448395Z",
          "shell.execute_reply.started": "2024-03-08T19:30:26.934584Z",
          "shell.execute_reply": "2024-03-08T19:30:43.447411Z"
        },
        "trusted": true,
        "id": "RDElAm4K2_Iq",
        "outputId": "66b615a7-a4cd-4968-a868-807a416cfd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**utils.py**"
      ],
      "metadata": {
        "id": "slMgE8ol2_Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#coding=utf-8\n",
        "import rouge\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "rouge = rouge.Rouge()\n",
        "def compute_rouge(source, target):\n",
        "\n",
        "    source, target = ' '.join(source), ' '.join(target)\n",
        "    try:\n",
        "        scores = rouge.get_scores(hyps=source, refs=target)\n",
        "        return {\n",
        "            'rouge-1': scores[0]['rouge-1']['f'],\n",
        "            'rouge-2': scores[0]['rouge-2']['f'],\n",
        "            'rouge-l': scores[0]['rouge-l']['f'],\n",
        "        }\n",
        "    except ValueError:\n",
        "        return {\n",
        "            'rouge-1': 0.0,\n",
        "            'rouge-2': 0.0,\n",
        "            'rouge-l': 0.0,\n",
        "        }\n",
        "\n",
        "\n",
        "def compute_rouges(sources, targets):\n",
        "    scores = {\n",
        "        'rouge-1': 0.0,\n",
        "        'rouge-2': 0.0,\n",
        "        'rouge-l': 0.0,\n",
        "    }\n",
        "    for source, target in zip(sources, targets):\n",
        "        score = compute_rouge(source, target)\n",
        "        for k, v in scores.items():\n",
        "            scores[k] = v + score[k]\n",
        "    return {k: v / len(targets) for k, v in scores.items()}\n",
        "\n",
        "\n",
        "def save_dataset(path, dataset):\n",
        "\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for sample in dataset:\n",
        "            f.write(sample + '\\n')\n",
        "\n",
        "\n",
        "def read_dataset(path):\n",
        "    dataset = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            # if i > 10:\n",
        "            #     break\n",
        "            line = line.strip()\n",
        "            line = json.loads(line)\n",
        "            dataset.append(line)\n",
        "    return dataset\n",
        "\n",
        "def save_model(output_model_file, model, optimizer):\n",
        "    os.makedirs(output_model_file, exist_ok=True)\n",
        "    output_model_file += 'pytorch_model.bin'\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, output_model_file)\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)  # cpu\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # gpu\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True  # consistent results on the cpu and gpu\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:43.4505Z",
          "iopub.execute_input": "2024-03-08T19:30:43.450862Z",
          "iopub.status.idle": "2024-03-08T19:30:45.310419Z",
          "shell.execute_reply.started": "2024-03-08T19:30:43.450809Z",
          "shell.execute_reply": "2024-03-08T19:30:45.309615Z"
        },
        "trusted": true,
        "id": "idAsVvc42_Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**modeling_transformer.py**"
      ],
      "metadata": {
        "id": "fZPBPIG52_Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless requreader2d by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import logging\n",
        "import tarfile\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
        "}\n",
        "CONFIG_NAME = 'bert_config.json'\n",
        "WEIGHTS_NAME = 'pytorch_model.bin'\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly diffretrievernt (and gives slightly diffretrievernt results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 hidden_dropout_prob=0.2,\n",
        "                 attention_probs_dropout_prob=0.2,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02):\n",
        "        \"\"\"Constructs BertConfig.\n",
        "\n",
        "        Args:\n",
        "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "        \"\"\"\n",
        "        if isinstance(vocab_size_or_config_json_file, str):\n",
        "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.num_attention_heads = num_attention_heads\n",
        "            self.hidden_act = hidden_act\n",
        "            self.intermediate_size = intermediate_size\n",
        "            self.hidden_dropout_prob = hidden_dropout_prob\n",
        "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \"or the path to a pretrained model config file (str)\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "try:\n",
        "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
        "except ImportError:\n",
        "    print(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\")\n",
        "\n",
        "\n",
        "    class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        query_layer_hat = torch.nn.functional.normalize(query_layer, dim=-1)\n",
        "        key_layer_hat = torch.nn.functional.normalize(key_layer, dim=-1)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer_hat, key_layer_hat.transpose(-1, -2))\n",
        "\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        #attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        #attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "        attention_softmax = attention_scores\n",
        "\n",
        "        # This is actually dropping out entreader2 tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_scores)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        B, n_head, n_tok, n_embd = query_layer.shape\n",
        "        context_layer /= n_embd\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer, attention_softmax\n",
        "\n",
        "    # def forward(self, hidden_states, attention_mask):\n",
        "    #     mixed_query_layer = self.query(hidden_states)\n",
        "    #     mixed_key_layer = self.key(hidden_states)\n",
        "    #     mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "    #     query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "    #     key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "    #     value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "    #     query_layer_hat = torch.nn.functional.normalize(query_layer, dim = -2)\n",
        "    #     key_layer_hat = torch.nn.functional.normalize(key_layer, dim = -2)\n",
        "\n",
        "    #     attention_scores = torch.matmul(key_layer_hat.transpose(-1, -2), value_layer)\n",
        "\n",
        "    #     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "    #    #attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "    #     attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "    #     # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "    #     attention_scores = attention_scores + attention_mask\n",
        "    #     # Normalize the attention scores to probabilities.\n",
        "    #     #attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "    #     #attention_softmax = attention_probs\n",
        "\n",
        "    #     # This is actually dropping out entreader2 tokens to attend to, which might\n",
        "    #     # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    #     #attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "    #     context_layer = torch.matmul(query_layer_hat, attention_scores)\n",
        "    #     B, n_tok, n_embd = query_layer.size()\n",
        "    #     attention_scores = context_layer / n_embd\n",
        "\n",
        "    #     context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "    #     new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "    #     context_layer = context_layer.view(*new_context_layer_shape)\n",
        "    #     return context_layer, attention_scores\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output, attention_probs = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output, attention_probs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n",
        "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output, attention_probs = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output, attention_probs\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        layer = BertLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states, attention_probs = layer_module(hidden_states, attention_mask)\n",
        "\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPredictionHeadTransform, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.transform_act_fn = ACT2FN[config.hidden_act] \\\n",
        "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertLMPredictionHead, self).__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but thretriever is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n",
        "                                 bert_model_embedding_weights.size(0),\n",
        "                                 bias=False)\n",
        "        self.decoder.weight = bert_model_embedding_weights\n",
        "        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertOnlyMLMHead, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyNSPHead, self).__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "class MyTransformer(nn.Module):\n",
        "    def __init__(self, dim, num_attention_heads, num_hidden_layers):\n",
        "        super(MyTransformer, self).__init__()\n",
        "        config = Config()\n",
        "        config.num_hidden_layers = num_hidden_layers\n",
        "        config.hidden_size = dim\n",
        "        config.num_attention_heads = num_attention_heads\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "    def forward(self, embedding_output, attention_mask, output_all_encoded_layers=False):\n",
        "\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "class Config():\n",
        "    def __init__(self):\n",
        "        self.attention_probs_dropout_prob = 0.2\n",
        "        self.hidden_act = \"relu\"\n",
        "        self.hidden_dropout_prob = 0.2\n",
        "        self.hidden_size = 768\n",
        "        self.initializer_range = 0.02\n",
        "        self.max_position_embeddings = 513\n",
        "        self.num_attention_heads = 16\n",
        "        # self.num_attention_heads = 12\n",
        "        # self.num_attention_heads = 8\n",
        "        self.num_hidden_layers = 1\n",
        "        self.type_vocab_size = 2\n",
        "        self.layer_norm_eps = 1e-05\n",
        "        self.intermediate_size = 3072\n",
        "\n",
        "class MultiModalMapping(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(MultiModalMapping, self).__init__()\n",
        "        self.dense = nn.Linear(dim, dim * 4)\n",
        "        self.dense_output = nn.Linear(dim * 4, dim)\n",
        "        self.intermediate_act_fn = ACT2FN['relu']\n",
        "        self.LayerNorm = BertLayerNorm(dim, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states0 = hidden_states\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        hidden_states = self.dense_output(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + hidden_states0)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "    # def __init__(self, dim):\n",
        "    #     super(MultiModalMapping, self).__init__()\n",
        "    #     self.fc1 = nn.Linear(dim, 4*dim)\n",
        "    #     self.fc2 = nn.Linear(4*dim, dim)\n",
        "    #     self.fc3 = nn.Linear(dim, dim)\n",
        "    #     self.fc4 = nn.Linear(dim, dim)\n",
        "    #     self.relu = nn.ReLU()\n",
        "    #     self.layer_norm = BertLayerNorm(dim)\n",
        "    #     self.dropout = nn.Dropout(0.5)\n",
        "    #     # nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=)\n",
        "    #     self.fake_conv = nn.Linear(3, 1)\n",
        "    #\n",
        "    #\n",
        "    # def forward(self, hidden_states):\n",
        "    #     hidden_states = self.layer_norm(hidden_states)\n",
        "    #     h1 = self.fc1(hidden_states)\n",
        "    #     h1 = self.dropout(h1)\n",
        "    #     h2 = self.layer_norm(self.fc2(h1))\n",
        "    #     # h3 = self.layer_norm(self.fc3(h2))\n",
        "    #     # h4 = self.layer_norm(self.fc4(h3))\n",
        "    #     h = self.relu(h2)\n",
        "    #     return h\n",
        "    #     # stack = torch.cat([h2.unsqueeze(dim=-1), h3.unsqueeze(dim=-1), h4.unsqueeze(dim=-1)], dim=3)\n",
        "    #     # h = self.fake_conv(stack)\n",
        "    #     # h = h.view(h.size(0), h.size(1), h.size(2))\n",
        "    #     # h = self.relu(h)\n",
        "    #     # return h\n",
        "\n",
        "\n",
        "class SemanticMatch(nn.Module):\n",
        "    def __init__(self, dim, layer_num):\n",
        "        super(SemanticMatch, self).__init__()\n",
        "        dpda_layer = DPDALayear(dim)\n",
        "        self.q_o = nn.ModuleList([copy.deepcopy(dpda_layer) for _ in range(layer_num)])\n",
        "        self.linear = nn.Linear(2 * dim, dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, q, o, mask_q, mask_o):\n",
        "        for layer_module in self.q_o:\n",
        "            q, o, q_weight, o_weight = layer_module(q, o, mask_q, mask_o)\n",
        "        q, _ = torch.max(q, dim=1)\n",
        "        o, _ = torch.max(o, dim=1)\n",
        "        q_o = self.get_vector(q, o)\n",
        "        return q_o, q_weight, o_weight\n",
        "\n",
        "    def get_vector(self, v1, v2):\n",
        "        p_b = torch.cat([v1, v2], dim=1)  # 2*dim\n",
        "        p_b = self.linear(p_b)  # dim\n",
        "        p_b = self.relu(p_b)\n",
        "        return p_b\n",
        "\n",
        "\n",
        "class DPDALayear(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(DPDALayear, self).__init__()\n",
        "        self.W_p = nn.Linear(2 * dim, dim)\n",
        "        self.W_q = nn.Linear(2 * dim, dim)\n",
        "        self.W_map = nn.Linear(dim, dim)\n",
        "\n",
        "\n",
        "    def forward(self, P, Q, p_mask=None, q_mask=None):\n",
        "        # P = self.W_map(P)\n",
        "        P_ori = P\n",
        "        Q_ori = Q\n",
        "        A = torch.matmul(P, Q.transpose(dim0=1, dim1=2))  # batch, l_p, l_q\n",
        "\n",
        "        if p_mask is not None:\n",
        "            p_mask = p_mask.float()\n",
        "            p_mask = 1 - p_mask\n",
        "            p_mask = p_mask * -10000.0\n",
        "            p_mask = p_mask.unsqueeze(dim=2)\n",
        "            p_mask = p_mask.expand_as(A)\n",
        "            A = A + p_mask\n",
        "            A = A.to(P.dtype)\n",
        "\n",
        "        if q_mask is not None:\n",
        "            q_mask = q_mask.float()\n",
        "            q_mask = 1 - q_mask\n",
        "            q_mask = q_mask * -10000.0\n",
        "            q_mask = q_mask.unsqueeze(dim=1)\n",
        "            q_mask = q_mask.expand_as(A)\n",
        "            A = A + q_mask\n",
        "            A = A.to(Q.dtype)\n",
        "\n",
        "        p_weight, _ = torch.max(A, dim=2)\n",
        "        q_weight, _ = torch.max(A, dim=1)\n",
        "        # if p_mask is not None:\n",
        "        #     p_weight *= p_mask\n",
        "        # if q_mask is not None:\n",
        "        #     q_weight *= q_mask\n",
        "\n",
        "\n",
        "        A_q = torch.softmax(A, dim=2)  # l_p, l_q\n",
        "        A_p = torch.softmax(A.transpose(dim0=1, dim1=2), dim=2)  # l_q, l_p\n",
        "\n",
        "        P_q = torch.matmul(A_q, Q)  # l_p, dim\n",
        "        Q_p = torch.matmul(A_p, P)  # l_q, dim\n",
        "\n",
        "        P_t = torch.cat([P_q, P], dim=2)  # l_p, 2*dim\n",
        "        Q_t = torch.cat([Q_p, Q], dim=2)  # l_q, 2*dim\n",
        "\n",
        "        Q = torch.matmul(A_p, P_t)  # l_q, 2*dim\n",
        "        P = torch.matmul(A_q, Q_t)  # l_p, 2*dim\n",
        "        P = P_ori + self.W_p(P)  # l_p, dim\n",
        "        layer_norm = nn.LayerNorm(normalized_shape=[P.size(-2), P.size(-1)],elementwise_affine=False)\n",
        "        P = layer_norm(P)\n",
        "        Q = Q_ori + self.W_q(Q)  # l_q, dim\n",
        "        layer_norm = nn.LayerNorm(normalized_shape=[Q.size(-2), Q.size(-1)],elementwise_affine=False)\n",
        "        Q = layer_norm(Q)\n",
        "        return P, Q, p_weight, q_weight\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:45.313466Z",
          "iopub.execute_input": "2024-03-08T19:30:45.313842Z",
          "iopub.status.idle": "2024-03-08T19:30:45.408474Z",
          "shell.execute_reply.started": "2024-03-08T19:30:45.313802Z",
          "shell.execute_reply": "2024-03-08T19:30:45.407588Z"
        },
        "trusted": true,
        "id": "6n6awzBs2_Is",
        "outputId": "23147f3d-ef51-48cc-94cf-3e035780b774"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**modeling_genmc.py**"
      ],
      "metadata": {
        "id": "blQfr-_i2_Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#coding=utf-8\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers.file_utils import ModelOutput\n",
        "import torch\n",
        "#from .modeling_transformer import MyTransformer, SexnticMatch\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "class GenMC(nn.Module):\n",
        "    def __init__(self, model_path, num_hidden_layers, alpha, beta):\n",
        "        super(GenMC, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        self.t5_model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "        dim = self.t5_model.config.d_model\n",
        "        self.option_linear = nn.Linear(dim, 1).to(device)\n",
        "        self.option_linear.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.semantic_matching = SemanticMatch(dim, num_hidden_layers).to(device)\n",
        "        self.semantic_matching.device = device\n",
        "        num_attention_heads = dim // 32    #original: // 64\n",
        "        self.transformer_laryer_de = MyTransformer(dim, num_attention_heads, num_hidden_layers).to(device)\n",
        "        self.transformer_laryer_de.device = device\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "        layer_num = self.t5_model.config.num_layers\n",
        "        layer_per_gpu = layer_num // n_gpu\n",
        "        layer_per_gpu_remainder = layer_num % n_gpu\n",
        "        device_map = {}\n",
        "        cur_layer = 0\n",
        "        for n in range(n_gpu):\n",
        "            device_map[n] = []\n",
        "            if n < layer_per_gpu_remainder:\n",
        "                layer_assigned = layer_per_gpu+1\n",
        "            else:\n",
        "                layer_assigned = layer_per_gpu\n",
        "\n",
        "            for i in range(layer_assigned):\n",
        "                device_map[n].append(cur_layer)\n",
        "                cur_layer += 1\n",
        "        self.t5_model.parallelize(device_map)\n",
        "\n",
        "\n",
        "    def forward(self, q_ids, q_mask, qo_ids, qo_mask, choice_num, clue_ids=None, clue_content_ids=None, clue_content_mask=None, answers=None):\n",
        "        self.choice_num = choice_num\n",
        "        if answers is not None and clue_ids is not None and clue_content_ids is not None and clue_content_mask is not None:\n",
        "            opt_score, output_sequences = self.get_option_score(q_ids, q_mask, qo_ids, qo_mask)\n",
        "            local_device = self.t5_model.device\n",
        "            t5_output = self.t5_model(input_ids=clue_content_ids.to(local_device), attention_mask=clue_content_mask.to(local_device),\n",
        "                                      labels=clue_ids.to(local_device))\n",
        "            loss_ans = t5_output.loss\n",
        "            loss = self.criterion(opt_score, answers)\n",
        "            return self.alpha * loss + self.beta * loss_ans\n",
        "        else:\n",
        "            opt_score, output_sequences = self.get_option_score(q_ids, q_mask, qo_ids, qo_mask)\n",
        "            return opt_score, output_sequences\n",
        "\n",
        "    def get_option_score(self, q_ids, q_mask, qo_ids, qo_mask):\n",
        "        local_device = self.t5_model.encoder.device\n",
        "        t5_output = self.t5_model.encoder(input_ids=qo_ids.to(local_device), attention_mask=qo_mask.to(local_device))\n",
        "        encoder_qo = t5_output[0]\n",
        "\n",
        "        t5_output = self.t5_model.encoder(input_ids=q_ids.to(local_device), attention_mask=q_mask.to(local_device))\n",
        "        encoder_q = t5_output[0]\n",
        "        local_device = self.t5_model.device\n",
        "        t5_output = self.t5_model.generate(\n",
        "            encoder_outputs=ModelOutput(last_hidden_state=encoder_q.to(local_device)),\n",
        "            attention_mask=q_mask.to(local_device),\n",
        "            do_sample=False,\n",
        "            output_hidden_states=True,\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "        output_sequences = t5_output.sequences\n",
        "        output_sequences = output_sequences[:, 1:].contiguous()\n",
        "        decoder_o = t5_output.decoder_hidden_states\n",
        "        decoder_o = [item[-1] for item in decoder_o]\n",
        "        decoder_o = torch.cat(decoder_o, dim=1)\n",
        "\n",
        "        output_sequences_mask1 = output_sequences != 0\n",
        "        output_sequences_mask2 = output_sequences != 1\n",
        "        output_sequences_mask = output_sequences_mask1 * output_sequences_mask2\n",
        "        output_sequences_mask = output_sequences_mask.long()\n",
        "        decoder_qo = torch.cat([encoder_q, decoder_o], dim=1)\n",
        "        output_sequences_mask = torch.cat([q_mask, output_sequences_mask], dim=1)\n",
        "        local_device = self.transformer_laryer_de.device\n",
        "        decoder_qo, _ = self.transformer_laryer_de(decoder_qo.to(local_device), output_sequences_mask.to(local_device))\n",
        "        output_sequences_mask_ex = output_sequences_mask.unsqueeze(dim=1)\n",
        "        output_sequences_mask_ex = output_sequences_mask_ex.expand(\n",
        "            [output_sequences_mask_ex.size(0), self.choice_num, output_sequences_mask_ex.size(-1)]).contiguous()\n",
        "        output_sequences_mask_ex = output_sequences_mask_ex.view(-1, output_sequences_mask.size(-1))\n",
        "        decoder_qo = decoder_qo.unsqueeze(dim=1)\n",
        "        decoder_qo = decoder_qo.expand(\n",
        "            [decoder_qo.size(0), self.choice_num, decoder_qo.size(-2), decoder_qo.size(-1)]).contiguous()\n",
        "        decoder_qo = decoder_qo.view(-1, decoder_qo.size(-2), decoder_qo.size(-1))\n",
        "        local_device = self.semantic_matching.device\n",
        "        semantic_vec, _, _ = self.semantic_matching(encoder_qo.to(local_device), decoder_qo.to(local_device),\n",
        "                                                    qo_mask.to(local_device), output_sequences_mask_ex.to(local_device))\n",
        "        local_device = self.option_linear.device\n",
        "        opt_score = self.option_linear(semantic_vec.to(local_device)).view(-1, self.choice_num)\n",
        "\n",
        "        return opt_score, output_sequences\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:45.409652Z",
          "iopub.execute_input": "2024-03-08T19:30:45.409936Z",
          "iopub.status.idle": "2024-03-08T19:30:51.264333Z",
          "shell.execute_reply.started": "2024-03-08T19:30:45.409912Z",
          "shell.execute_reply": "2024-03-08T19:30:51.26344Z"
        },
        "trusted": true,
        "id": "cM6w6UMF2_Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**run_genmc.py**"
      ],
      "metadata": {
        "id": "Ec05TC9S2_Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "from transformers import T5Tokenizer\n",
        "from tqdm import trange\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "#from utils import compute_rouges, save_dataset, read_dataset, set_seed, save_model\n",
        "#from model.modeling_genmc import GenMC\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "print(\"************Torch version:\",torch.__version__)\n",
        "print(\"************Is CUDA enabled?\",torch.cuda.is_available())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:51.266587Z",
          "iopub.execute_input": "2024-03-08T19:30:51.267002Z",
          "iopub.status.idle": "2024-03-08T19:30:51.321206Z",
          "shell.execute_reply.started": "2024-03-08T19:30:51.266976Z",
          "shell.execute_reply": "2024-03-08T19:30:51.320115Z"
        },
        "trusted": true,
        "id": "lujqEudq2_Iu",
        "outputId": "8c43f28b-3861-4b06-84f3-033e8f13ef9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "************Torch version: 2.1.2\n************Is CUDA enabled? True\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF8wqNpO2_Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_feature(samples, max_source_length, max_len_gen, choice_num, external_sent_num=None):\n",
        "    sep = ' \\\\n '\n",
        "    output_clue = []\n",
        "    answers = []\n",
        "    input_ids_q, attention_mask_q = [], []\n",
        "    input_ids_qo, attention_mask_qo = [], []\n",
        "    input_clue_content_ids, attention_mask_clue_content = [], []\n",
        "    for sample in samples:\n",
        "        if 'answerKey' in sample:\n",
        "            answerKey = sample['answerKey']\n",
        "        else:\n",
        "            answerKey = \"A\"\n",
        "        question = sample['question']['stem']\n",
        "        while len(sample['question']['choices']) < choice_num:\n",
        "            sample['question']['choices'].append({\"text\": \"error\", \"para\": \"\", \"label\":chr(ord('A')+len(sample)-1)})\n",
        "\n",
        "        content_for_clue = \"\"\n",
        "        for o_i, (opt, opt_name) in enumerate(zip(sample['question']['choices'], 'ABCDEFGH'[:choice_num])):\n",
        "            option = opt['text']\n",
        "            content = \"\"\n",
        "            if external_sent_num is not None and 'para' in opt:\n",
        "                para = opt[\"para\"]\n",
        "                if isinstance(para, list):\n",
        "                    if len(para) > external_sent_num:\n",
        "                        #print(\"yes\")\n",
        "                        para = para[:external_sent_num]\n",
        "\n",
        "                    content = sep + \" \".join(para)\n",
        "                    if option == answerKey:\n",
        "                        print(\"hey\")\n",
        "                        content_for_clue = sep + \" \".join(para)\n",
        "                elif isinstance(para, str):\n",
        "                    para = para.split(\".\")\n",
        "                    if len(para) > external_sent_num:\n",
        "                        para = para[:external_sent_num]\n",
        "                    content = sep + \" \".join(para)\n",
        "                    if option == answerKey:\n",
        "                        print(\"hi\")\n",
        "                        content_for_clue = sep + \" \".join(para)\n",
        "                else:\n",
        "                    print('lack retrieval')\n",
        "                    # exit(0)\n",
        "            input_ids_qo.append(question + sep + option + content)\n",
        "\n",
        "\n",
        "        input_ids_q.append(question + sep)\n",
        "        input_clue_content_ids.append(question + sep + content_for_clue)\n",
        "\n",
        "        if answerKey in '123456':\n",
        "            answer = ord(answerKey) - ord('1')\n",
        "        else:\n",
        "            answer = ord(answerKey) - ord('A')\n",
        "        answers.append(answer)\n",
        "        output_clue.append(sample['question']['choices'][answer]['text'])\n",
        "\n",
        "    def tokenizer_fun(input_ids, max_len):\n",
        "        encoding = tokenizer(input_ids,\n",
        "                             padding='longest',\n",
        "                             max_length=max_len,\n",
        "                             truncation=True,\n",
        "                             return_tensors=\"pt\")\n",
        "        ids = encoding.input_ids.to(device)\n",
        "        mask = encoding.attention_mask.to(device)\n",
        "        return ids, mask\n",
        "\n",
        "    q_ids, q_mask = tokenizer_fun(input_ids_q, max_source_length)\n",
        "    qo_ids, qo_mask = tokenizer_fun(input_ids_qo, max_source_length)\n",
        "    clue_content_ids, clue_content_mask = tokenizer_fun(input_clue_content_ids, max_source_length)\n",
        "    clue_ids, _ = tokenizer_fun(output_clue, max_len_gen)\n",
        "    clue_ids = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in\n",
        "        clue_ids\n",
        "    ]\n",
        "    clue_ids = torch.tensor(clue_ids, dtype=torch.long).to(device)\n",
        "    answers = torch.tensor(answers, dtype=torch.long).to(device)\n",
        "    return q_ids, q_mask, qo_ids, qo_mask, clue_ids, clue_content_ids, clue_content_mask, answers, output_clue\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, test_examples, tokenizer, eval_batch_size, choice_num, max_len, max_len_gen, external_sent_num):\n",
        "    count, count_right = 0, 0\n",
        "    results = []\n",
        "    model.eval()\n",
        "    step_count = len(test_examples) // eval_batch_size\n",
        "    if step_count * eval_batch_size < len(test_examples):\n",
        "        step_count += 1\n",
        "    step_trange = trange(step_count)\n",
        "    sources, targets = [], []\n",
        "    for step in step_trange:\n",
        "        beg_index = step * eval_batch_size\n",
        "        end_index = min((step + 1) * eval_batch_size, len(test_examples))\n",
        "        batch_example = [example for example in test_examples[beg_index:end_index]]\n",
        "        q_ids, q_mask, qo_ids, qo_mask, clue_ids, clue_content_ids, clue_content_mask, answers, output_clue = get_input_feature(batch_example,\n",
        "                                                                                           max_len, max_len_gen,\n",
        "                                                                                           args_choice_num,\n",
        "                                                                                           external_sent_num)\n",
        "        scores, output_sequences = model(q_ids, q_mask, qo_ids, qo_mask, choice_num)\n",
        "\n",
        "        scores = scores.cpu().detach().tolist()\n",
        "        answers = answers.cpu().detach().tolist()\n",
        "        p_anss = []\n",
        "        for p, a, example in zip(scores, answers, batch_example):\n",
        "            p_ans = p.index(max(p))\n",
        "            p_anss.append(example['question']['choices'][p_ans]['label'])\n",
        "            if p_ans == a:\n",
        "                count_right += 1\n",
        "            count += 1\n",
        "        for sample, p_ans in zip(batch_example, p_anss):\n",
        "            qid = sample['id']\n",
        "            results.append(qid + \",\" + p_ans)\n",
        "        predicts = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
        "        sources += predicts\n",
        "        targets += output_clue\n",
        "\n",
        "    rouge_score = compute_rouges(sources, targets)['rouge-l']\n",
        "\n",
        "    return count_right / count, rouge_score, results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:51.322749Z",
          "iopub.execute_input": "2024-03-08T19:30:51.323058Z",
          "iopub.status.idle": "2024-03-08T19:30:51.363529Z",
          "shell.execute_reply.started": "2024-03-08T19:30:51.323032Z",
          "shell.execute_reply": "2024-03-08T19:30:51.36246Z"
        },
        "trusted": true,
        "id": "a7lKivZl2_Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = read_dataset(\"/kaggle/input/openbkqa/train.jsonl\")\n",
        "dev_examples = read_dataset(\"/kaggle/input/openbkqa/dev.jsonl\")\n",
        "test_examples = read_dataset(\"/kaggle/input/openbkqa/test.jsonl\")\n",
        "print(train_examples[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:51.364966Z",
          "iopub.execute_input": "2024-03-08T19:30:51.365368Z",
          "iopub.status.idle": "2024-03-08T19:30:53.691573Z",
          "shell.execute_reply.started": "2024-03-08T19:30:51.365335Z",
          "shell.execute_reply": "2024-03-08T19:30:53.688606Z"
        },
        "trusted": true,
        "id": "Bipcd0Ya2_Iv",
        "outputId": "7d6e4aa7-fe71-447b-bd20-6cdb7a97645d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'id': '7-980', 'question': {'stem': 'The sun is responsible for', 'choices': [{'text': 'puppies learning new tricks', 'label': 'A', 'para': ['seasonal changes are made in response to changes in the environment  .', ' skills are learned characteristics  .', ' using tools is a learned characteristic  .', ' doing chores is a learned characteristic  .', ' preferences are generally learned characteristics  .', ' animals learn some behaviors from watching their parents  .', ' An example of seed dispersal is animals eating seeds  .', ' some animals shed fur in warm weather  .', ' if two animals eat the same prey then those animals compete for that pey  .', ' if an animal is trained to do something then that something is a learned behavior  .', ' the pupa stage is a stage in the metamorphosis process of some animals  .', ' An example of seed dispersal is is an animal gathering seeds  .', ' the sun sets in the west  .', ' if seeds stick to the fur of an animal then that seed will be transported by the animal  .', ' a tree can be replaced by planting a new tree  .', ' smell is used for finding food by some animals  .', ' a seed is used for storing food for a new plant  .', ' sweat is used for adjusting to hot temperatures by some animals  .', ' the Earth revolves around the sun  .', ' migration is when animals move themselves from a cooler climate to a warmer climate for the winter  .', ' if it is night then the sun has set  .', ' if a habitat can no longer support animals then those animals will move to another area  .', ' the Sun is the star that is closest to Earth  .', ' coloration is used to find a mate by some animals  .', ' a new season occurs four times per year  .', ' changes in an environment cause animals to adapt to survive  .', ' An example of recycling is using an object to make a new object  .', ' if an animal takes a resource from another animal then that animal competes for resources successfully  .', ' a new season occurs once per three months  .', ' panting is when an animal hangs its tongue out of its mouth to adjust to hot temperatures  .', ' the first quarter phase of the moon occurs after the new moon  .', ' In the food chain process an animal has the role of consumer which eats producers for food  .', ' new land can be formed by volcanoes erupting by lava cooling  .', ' echolocation is when some animals detect objects by hearing echoes by emitting sound  .', ' a solar eclipse could only happen during the new moon  .', ' if two animals have the same food source then those two animals compete for food  .', ' if a new predator begins eating prey then the population of that prey will decrease  .', ' feeders attract animals to a location  .', ' Earth is the planet that is third closest to the Sun  .', ' producing light is used for attracting prey by some deep sea animals  .', ' the sun is a source of light called sunlight  .', ' bacteria can help digest food in humans  .', ' Pluto is the planet that is ninth closest to the Sun  .', ' shivering is when an animal creates heat by shaking to keep the body warm  .', ' the sun is a source of heat called sunlight  .', ' In the food chain process an animal has the role of consumer which eats other animals for food .']}, {'text': 'children growing up and getting old', 'label': 'B', 'para': ['seasonal changes are made in response to changes in the environment  .', ' eating food is used to get nutrients by living things  .', ' a plant requires sunlight to grow  .', ' all living things grow  .', ' a plant requires photosynthesis to grow  .', ' if something is in the sunlight then that something will absorb solar energy  .', ' as the size of a leaf increases , the amount of sunlight absorbed by that leaf will increase  .', ' the sun transfers solar energy from itself to the Earth through sunlight  .', ' the amount of daylight is greatest on the summer solstice  .', ' a plants require water for to grow  .', ' the sun is the source of solar energy called sunlight  .', ' a plant requires soil for to grow  .', ' the sun is a source of light called sunlight  .', ' a plant requires water to grow  .', ' as temperature during the day increases , the temperature in an environment will increase  .', ' good bacteria grow on the skin  .', ' a desert environment usually has a lot of sunlight  .', ' the sun sets in the west  .', ' the sun is a source of heat called sunlight  .', ' evaporation is when water is drawn back up into the air in the water cycle  .', ' if something is outside during the day then that something will receive sunlight  .', \" a compass 's needle lines up with Earth 's magnetic poles  .\", ' a plant requires sunlight for photosynthesis  .', ' An example of evaporation is a body of water drying up by absorbing heat energy  .', ' if a substance absorbs solar energy then that substance will increase in temperature  .', ' electrical current running through a wire causes that wire to heat up  .', ' the sun transfers heat energy from itself to the planets through sunlight  .', ' an animal requires nutrients to grow and heal  .', ' the sun is the source of energy for life on Earth  .', ' a living thing requires nutrients to grow  .', ' the amount of daylight is greatest in the summer  .', ' soil is a renewable resource for growing plants  .', ' a leaf absorbs sunlight to perform photosynthesis  .', ' the Earth revolves around the sun  .', ' a solar panel converts sunlight into electricity  .', ' if it is night then the sun has set  .', ' a plant light is used for help plants by mimicking sunlight  .', ' the Sun is the star that is closest to Earth  .', ' as flatness of a leaf increases , the amount of sunlight that leaf can absorb will increase  .', ' a plant requires a specific climate to grow and survive  .', ' solar energy is an inexhaustible resource  .', \" good bacteria grow on a human 's skin  .\", ' summer is when a hemisphere is tilted towards the sun  .', ' Earth is the planet that is third closest to the Sun  .', ' clear weather means sunny weather  .', ' as temperature increases , the ability of that liquid to dissolve solids will increase  .', ' Pluto is the planet that is ninth closest to the Sun  .', ' the sun is the source of energy for physical cycles on Earth  .', ' sunlight produces heat  .', ' the Earth rotating on its axis causes the sun to appear to move across the sky during the day .']}, {'text': 'flowers wilting in a vase', 'label': 'C', 'para': ['seasonal changes are made in response to changes in the environment  .', ' a flower is a source of nectar  .', ' flowers are a source of fruit  .', ' as the number of pollinators attracted to a flower increases , the ability of that flower to reproduce will increase  .', ' pollination is when wind carry pollen from one flower to another flower  .', ' a plant light is used for help plants by mimicking sunlight  .', ' a leaf absorbs sunlight to perform photosynthesis  .', ' a plant requires sunlight for photosynthesis  .', ' chlorophyll is used for absorbing light energy by plants  .', ' a flower produces pollen and seeds  .', ' nectar is used for attracting pollinators by plants  .', ' some flowers become fruits  .', ' a plant requires sunlight to grow  .', \" a flower 's purpose is to produce seeds  .\", ' the sun sets in the west  .', ' birds are a vehicle for spreading the seeds of a plant  .', ' as the size of a flower increases , the number of pollinators it will attract increases  .', ' a solar panel converts sunlight into electricity  .', ' the Earth revolves around the sun  .', ' if it is night then the sun has set  .', ' if a tree falls then sunlight becomes available to the surrounding plants  .', ' the Sun is the star that is closest to Earth  .', ' the sun transfers solar energy from itself to the Earth through sunlight  .', ' as the amount of fragrance of a flower increases , the number of pollinators it will attract increases  .', ' plants are a source of oxygen through photosynthesis  .', ' Earth is the planet that is third closest to the Sun  .', ' green plants provide food for consumers by performing photosynthesis  .', ' the sun is a source of light called sunlight  .', ' Pluto is the planet that is ninth closest to the Sun  .', ' when pollen sticks to a hummingbird , that pollen will move to where the hummingbird moves  .', ' the sun is a source of heat called sunlight  .', ' the sun is the source of energy for life on Earth  .', ' the sun transfers heat energy from itself to the planets through sunlight  .', ' the sun is located directly overhead at noon  .', \" seed dispersal has a positive impact on a plant 's reproduction  .\", ' Earth orbiting the Sun causes seasons to change  .', ' the sun is the source of solar energy called sunlight  .', ' the sun is the source of energy for physical cycles on Earth  .', ' as the size of a leaf increases , the amount of sunlight absorbed by that leaf will increase  .', ' as flatness of a leaf increases , the amount of sunlight that leaf can absorb will increase  .', ' summer is when a hemisphere is tilted towards the sun  .', ' soil is a renewable resource for growing plants  .', ' the sun setting occurs once per day  .', ' looking directly at an eclipse of the Sun causes harm to the eyes .']}, {'text': 'plants sprouting, blooming and wilting', 'label': 'D', 'para': ['seeds may sprout when buried in soil  .', ' seasonal changes are made in response to changes in the environment  .', ' the sun sets in the west  .', ' the Earth revolves around the sun  .', ' if it is night then the sun has set  .', ' a leaf absorbs sunlight to perform photosynthesis  .', ' a plant requires sunlight for photosynthesis  .', ' a plant requires sunlight to grow  .', ' a plant light is used for help plants by mimicking sunlight  .', ' soil is a renewable resource for growing plants  .', ' the Sun is the star that is closest to Earth  .', ' if a tree falls then sunlight becomes available to the surrounding plants  .', ' Earth is the planet that is third closest to the Sun  .', ' as the size of a leaf increases , the amount of sunlight absorbed by that leaf will increase  .', ' the sun is a source of light called sunlight  .', ' plants are a source of oxygen through photosynthesis  .', ' Pluto is the planet that is ninth closest to the Sun  .', ' green plants provide food for consumers by performing photosynthesis  .', ' the sun is a source of heat called sunlight  .', ' a plant requires photosynthesis to grow  .', ' the sun is the source of energy for life on Earth  .', ' nectar is used for attracting pollinators by plants  .', ' the sun is located directly overhead at noon  .', ' birds are a vehicle for spreading the seeds of a plant  .', ' Earth orbiting the Sun causes seasons to change  .', ' the sun is the source of energy for physical cycles on Earth  .', ' soil contains nutrients for plants  .', ' the sun is the source of solar energy called sunlight  .', ' a flower produces pollen and seeds  .', ' summer is when a hemisphere is tilted towards the sun  .', ' when a plant grows , the number of the leaves of that plant may increase  .', ' the sun setting occurs once per day  .', ' a leaf performs photosynthesis  .', ' special tissues in plants transport minerals throughout the plant  .', ' chlorophyll is used for absorbing light energy by plants  .', ' planting native plants has a positive impact on an ecosystem  .', ' pollination is when wind carry pollen from one flower to another flower  .', ' transpiration usually happens in the leaves of a plant  .', ' looking directly at an eclipse of the Sun causes harm to the eyes  .', ' a plant stem is the vehicle for transporting water and food from roots to the rest of the plant  .', ' winter is when a hemisphere is tilted away from the sun  .', \" a flower 's purpose is to produce seeds  .\", ' the sun rising and setting causes cycles of day and night  .', ' plant cells can perform photosynthesis  .', ' as the amount of water received by a plant increases , that plant will usually grow  .', ' seed dispersal has a positive impact on a plant  .', ' xylem carries water from the roots of a plant to the leaves of a plant  .', ' specialized tissues at the ends of plant stems are used for growing taller by plants  .', ' seed dispersal is when the seeds of a plant are spread from the parent plant to another area .']}]}, 'answerKey': 'D'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:53.693207Z",
          "iopub.execute_input": "2024-03-08T19:30:53.693641Z",
          "iopub.status.idle": "2024-03-08T19:30:53.70206Z",
          "shell.execute_reply.started": "2024-03-08T19:30:53.693604Z",
          "shell.execute_reply": "2024-03-08T19:30:53.700796Z"
        },
        "trusted": true,
        "id": "mIUZgIZU2_Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:50:48.624476Z",
          "iopub.execute_input": "2024-03-08T19:50:48.625543Z",
          "iopub.status.idle": "2024-03-08T19:50:48.662981Z",
          "shell.execute_reply.started": "2024-03-08T19:50:48.625505Z",
          "shell.execute_reply": "2024-03-08T19:50:48.66203Z"
        },
        "trusted": true,
        "id": "ax8OWdnq2_Iv",
        "outputId": "bfb6b1c2-4e5d-4577-a702-11cf077f125d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/openbook/dev.jsonl\n/kaggle/input/openbook/test.jsonl\n/kaggle/input/openbook/train.jsonl\n/kaggle/input/openbkqa/dev.jsonl\n/kaggle/input/openbkqa/test.jsonl\n/kaggle/input/openbkqa/train.jsonl\n/kaggle/input/genmccheckpoints/checkpointspytorch_model.bin\n/kaggle/input/genmccheckpoints/results/lr_0.0001_seed_1_bs_8_ga_1_layer_num_1_alpha_0.5_beta_1/dev.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'arc_challenge'\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args_model_path = 'google/flan-t5-base'\n",
        "    args_choice_num = 4\n",
        "    args_train_batch_size = 8 # total batch size for training\n",
        "    args_eval_batch_size = 8 # total batch size for eval\n",
        "    args_gradient_accumulation_steps = 1 # Number of updates steps to accumulate before performing a backward/update pass\n",
        "    args_max_len = 256\n",
        "    args_max_len_gen = 64\n",
        "    args_lr = 1e-4\n",
        "    args_epoch_num = 30\n",
        "    args_num_hidden_layers = 1\n",
        "    args_alpha = 0.5\n",
        "    args_beta = 1\n",
        "    args_seed = 1\n",
        "    args_external_sent_num = 10\n",
        "    args_gpu = \"0\"\n",
        "    args_init_checkpoint = '/kaggle/input/genmccheckpoints/checkpointspytorch_model.bin'\n",
        "\n",
        "    file_name = f'lr_{args_lr}_seed_{args_seed}_bs_{args_train_batch_size}_ga_{args_gradient_accumulation_steps}_layer_num_{args_num_hidden_layers}_alpha_{args_alpha}_beta_{args_beta}'\n",
        "    output_model_path = './outputs/' + file_name + \"/\"\n",
        "    path_save_result = './results/' + file_name + \"/\"\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args_gpu\n",
        "    os.makedirs(path_save_result, exist_ok=True)\n",
        "    set_seed(args_seed)\n",
        "    train_examples = read_dataset(\"/kaggle/input/openbkqa/train.jsonl\")\n",
        "    dev_examples = read_dataset(\"/kaggle/input/openbkqa/dev.jsonl\")\n",
        "    test_examples = read_dataset(\"/kaggle/input/openbkqa/test.jsonl\")\n",
        "\n",
        "#     print(json.dumps({\"lr\": args.lr, \"model\": args.model_path, \"seed\": args.seed,\n",
        "#                       \"bs\": args.train_batch_size,\n",
        "#                       'gradient_accumulation_steps': args.gradient_accumulation_steps,\n",
        "#                       \"epoch\": args.epoch_num,\n",
        "#                       \"train_path\": args.data_path_train,\n",
        "#                       \"dev_path\": args.data_path_dev,\n",
        "#                       \"test_path\": args.data_path_test,\n",
        "#                       \"train_size\": len(train_examples),\n",
        "#                       \"dev_size\": len(dev_examples),\n",
        "#                       \"test_size\": len(test_examples),\n",
        "#                       'num_hidden_layers': args.num_hidden_layers,\n",
        "#                       'external_sent_num': args.external_sent_num,\n",
        "#                       \"alpha\": args.alpha, \"beta\": args.beta}, indent=2))\n",
        "\n",
        "    train_batch_size = args_train_batch_size // args_gradient_accumulation_steps\n",
        "    tokenizer = T5Tokenizer.from_pretrained(args_model_path)\n",
        "    model = GenMC(args_model_path, args_num_hidden_layers, args_alpha, args_beta)\n",
        "    if args_init_checkpoint is not None:\n",
        "        checkpoint = torch.load(args_init_checkpoint, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    test_acc, test_rouge_score, results_test = eval(model, test_examples, tokenizer, args_eval_batch_size,\n",
        "                                                    args_choice_num, args_max_len, args_max_len_gen,\n",
        "                                                    args_external_sent_num)\n",
        "    print(test_acc, test_rouge_score, results_test)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args_lr, weight_decay=0.01)\n",
        "\n",
        "    step_count, step_all, early_stop = 0, 0, 0\n",
        "    best_dev_rouge_score, best_test_rouge_score = 0, 0\n",
        "    tr_loss, nb_tr_steps = 0, 0\n",
        "    print(\"here1\")\n",
        "    best_dev_acc, _, _ = eval(model, dev_examples, tokenizer, args_eval_batch_size, args_choice_num, args_max_len,\n",
        "                              args_max_len_gen, args_external_sent_num)\n",
        "    print('best_dev_acc:',best_dev_acc)\n",
        "    best_test_acc = 0\n",
        "\n",
        "    print(\"here2\")\n",
        "    log = []\n",
        "    x = []\n",
        "    y_dev = []\n",
        "    y_train = []\n",
        "\n",
        "    for epoch in range(args_epoch_num):\n",
        "        x.append(epoch)\n",
        "        early_stop += 1\n",
        "        order = list(range(len(train_examples)))\n",
        "        random.seed(args_seed + epoch)\n",
        "        random.shuffle(order)\n",
        "        model.train()\n",
        "        step_count = len(train_examples) // train_batch_size\n",
        "        if step_count * train_batch_size < len(train_examples):\n",
        "            step_count += 1\n",
        "        step_trange = trange(step_count)\n",
        "        for step in step_trange:\n",
        "            step_all += 1\n",
        "            beg_index = step * train_batch_size\n",
        "            end_index = min((step + 1) * train_batch_size, len(train_examples))\n",
        "            order_index = order[beg_index:end_index]\n",
        "            batch_example = [train_examples[index] for index in order_index]\n",
        "            q_ids, q_mask, qo_ids, qo_mask, clue_ids, clue_content_ids, clue_content_mask, answers, output_clue = get_input_feature(\n",
        "                batch_example,\n",
        "                max_source_length=args_max_len,\n",
        "                max_len_gen=args_max_len_gen,\n",
        "                choice_num=args_choice_num,\n",
        "                external_sent_num=args_external_sent_num)\n",
        "            loss = model(q_ids, q_mask, qo_ids, qo_mask, args_choice_num, clue_ids, clue_content_ids, clue_content_mask, answers)\n",
        "\n",
        "            loss = loss.mean()\n",
        "            log.append(loss.item())\n",
        "            tr_loss += loss.item()\n",
        "            nb_tr_steps += 1\n",
        "            loss = loss / args_gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "            if (step + 1) % args_gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                # scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            loss_show = ' Epoch:' + str(epoch) + \" loss:\" + str(round(tr_loss / nb_tr_steps, 4))\n",
        "            step_trange.set_postfix_str(loss_show)\n",
        "\n",
        "        train_acc, train_rouge_score, results_dev = eval(model, train_examples, tokenizer, args_eval_batch_size,\n",
        "                                                     args_choice_num, args_max_len, args_max_len_gen,\n",
        "                                                     args_external_sent_num)\n",
        "\n",
        "        dev_acc, dev_rouge_score, results_dev = eval(model, dev_examples, tokenizer, args_eval_batch_size,\n",
        "                                                     args_choice_num, args_max_len, args_max_len_gen,\n",
        "                                                     args_external_sent_num)\n",
        "        print('train_acc', train_acc)\n",
        "        print('dev_acc:', dev_acc)\n",
        "        y_dev.append(dev_acc)\n",
        "        y_train.append(train_acc)\n",
        "\n",
        "        if dev_acc > best_dev_acc:\n",
        "            save_dataset(path_save_result + '/dev.csv', results_dev)\n",
        "            early_stop = 0\n",
        "#             test_acc, test_rouge_score, results_test = eval(model, test_examples, tokenizer, args_eval_batch_size,\n",
        "#                                                             args_choice_num, args_max_len, args_max_len_gen,\n",
        "#                                                             args_external_sent_num)\n",
        "\n",
        "            #save_dataset(path_save_result + '/test.csv', results_test)\n",
        "            #best_dev_acc, best_test_acc, best_dev_rouge_score, best_test_rouge_score = dev_acc, test_acc, dev_rouge_score, test_rouge_score\n",
        "            best_dev_acc,best_dev_rouge_score = dev_acc, dev_rouge_score\n",
        "\n",
        "            save_model('./checkpoints', model, optimizer)\n",
        "            print('new best dev acc:', dev_acc, 'rouge:', dev_rouge_score)\n",
        "\n",
        "        if early_stop >= 10:\n",
        "            break\n",
        "\n",
        "    print('best dev acc:', best_dev_acc,\n",
        "          'best_dev_rouge_score:', best_dev_rouge_score)\n",
        "    plt.plot(log)\n",
        "    plt.xlabel('step')\n",
        "    plt.ylabel('loss')\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:52:46.66577Z",
          "iopub.execute_input": "2024-03-08T19:52:46.666245Z",
          "iopub.status.idle": "2024-03-08T19:54:18.407893Z",
          "shell.execute_reply.started": "2024-03-08T19:52:46.666215Z",
          "shell.execute_reply": "2024-03-08T19:54:18.406229Z"
        },
        "trusted": true,
        "id": "xt6y6kqb2_Iv",
        "outputId": "86ff9f22-b7c0-43b4-83d6-b8a13f792af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n100%|██████████| 63/63 [00:46<00:00,  1.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "0.69 0.3813938560948636 ['8-343,C', '1129,C', '880,C', '7-999,C', '8-464,C', '9-794,C', '9-1163,C', '9-322,B', '7-1140,C', '7-903,A', '7-511,B', '9-937,B', '8-201,C', '1618,A', '758,C', '7-414,D', '9-675,C', '9-163,A', '1032,C', '889,C', '1160,D', '9-298,A', '1189,A', '8-395,D', '7-238,B', '7-372,A', '8-35,B', '9-271,B', '9-409,A', '530,A', '1426,C', '8-466,D', '1577,B', '8-257,C', '378,B', '8-41,A', '9-540,A', '266,C', '1309,D', '7-1197,D', '7-891,A', '1180,A', '1204,A', '7-52,A', '1759,A', '9-655,D', '132,A', '8-79,A', '1835,C', '9-149,C', '695,C', '8-179,D', '7-50,A', '508,A', '1674,A', '163,C', '7-49,B', '8-393,D', '788,C', '9-29,A', '9-368,C', '7-671,B', '1272,C', '648,C', '9-1180,B', '9-227,C', '1582,A', '8-125,A', '1923,B', '9-229,D', '1702,B', '8-260,B', '9-491,C', '75,B', '1215,C', '8-93,C', '7-988,A', '9-1139,C', '1545,A', '7-664,B', '8-53,C', '7-1044,A', '7-1122,A', '9-79,B', '7-157,D', '9-1164,D', '8-63,D', '8-308,C', '326,C', '1184,A', '359,A', '9-350,D', '7-140,D', '591,A', '7-391,C', '1672,C', '9-464,C', '9-983,B', '9-179,D', '7-942,C', '7-100,B', '9-30,B', '1709,A', '8-491,D', '44,B', '1023,D', '1911,B', '429,A', '8-49,D', '520,C', '7-1128,A', '7-394,C', '9-1166,B', '7-884,C', '9-501,A', '9-757,B', '7-725,D', '1300,A', '9-230,A', '9-988,B', '9-393,A', '7-823,A', '9-24,A', '570,D', '9-124,B', '9-199,B', '767,A', '28,C', '9-1134,D', '9-1030,A', '9-18,A', '8-378,D', '7-677,C', '9-786,A', '9-463,D', '7-71,A', '9-1053,D', '9-437,D', '1787,B', '7-107,A', '769,D', '9-73,D', '9-1194,D', '9-416,D', '470,D', '1297,D', '8-346,A', '7-807,A', '8-463,A', '9-110,A', '1611,C', '9-942,D', '9-1102,B', '9-774,C', '8-333,A', '9-573,C', '1955,A', '8-45,B', '9-674,B', '898,C', '7-1159,C', '568,A', '9-877,B', '406,C', '7-1132,A', '7-479,C', '609,A', '1568,C', '9-418,C', '7-1050,B', '9-510,C', '9-519,A', '9-637,C', '473,A', '8-445,D', '9-575,B', '7-284,B', '8-135,A', '397,C', '9-32,D', '48,C', '8-69,C', '9-159,C', '9-317,C', '423,D', '8-304,A', '785,B', '9-1087,C', '485,C', '9-908,C', '1231,C', '810,D', '158,B', '7-445,A', '1502,C', '1200,B', '437,A', '8-205,A', '9-270,C', '8-130,A', '229,A', '9-390,D', '8-107,C', '7-527,C', '7-333,C', '9-44,C', '7-160,D', '1942,D', '9-597,B', '9-35,A', '1161,B', '7-171,A', '1139,D', '1924,B', '9-440,B', '9-528,C', '170,B', '395,A', '9-633,D', '9-504,A', '8-192,B', '7-1108,B', '7-852,A', '761,D', '8-318,B', '636,A', '7-444,D', '8-57,A', '9-187,B', '1345,A', '8-59,D', '178,C', '9-1186,A', '82,C', '8-165,C', '404,B', '279,D', '9-532,C', '268,C', '7-1018,D', '1756,A', '1137,C', '7-203,D', '745,B', '7-902,A', '1095,A', '7-163,A', '9-858,C', '1530,D', '9-993,D', '8-340,B', '3,B', '1074,C', '9-431,A', '9-638,B', '9-352,D', '226,B', '9-132,A', '9-222,D', '9-105,D', '7-459,B', '9-881,C', '280,C', '187,D', '8-253,C', '9-482,D', '496,A', '630,D', '9-16,D', '7-986,C', '7-787,C', '9-181,A', '1240,B', '474,D', '1274,A', '1531,D', '8-321,B', '1321,C', '9-51,D', '7-685,D', '7-59,A', '7-270,D', '7-736,A', '8-186,D', '224,B', '8-206,C', '8-190,A', '7-334,B', '9-853,B', '8-367,A', '1047,A', '9-454,C', '1572,C', '8-373,B', '9-772,C', '1852,A', '9-1090,D', '7-769,C', '9-478,D', '448,D', '7-417,B', '7-108,D', '1506,A', '1712,B', '8-312,D', '9-776,B', '8-279,C', '9-621,B', '1823,D', '9-735,B', '7-1170,B', '1500,A', '342,B', '7-356,D', '78,A', '9-520,C', '7-653,C', '1112,A', '9-152,B', '9-552,B', '7-262,A', '7-683,D', '276,B', '7-855,C', '664,D', '9-883,C', '9-550,C', '8-493,D', '9-257,C', '1239,D', '869,A', '7-1105,A', '597,D', '385,B', '1301,B', '9-893,D', '9-369,B', '9-1026,A', '7-424,C', '9-259,A', '9-783,C', '1088,B', '1387,C', '7-1062,A', '676,C', '1998,B', '1698,A', '490,A', '844,A', '1795,B', '1508,B', '9-289,D', '9-668,C', '7-364,A', '1271,A', '9-1117,D', '35,B', '1660,B', '7-710,B', '8-52,B', '9-1167,D', '8-43,D', '9-57,B', '1411,B', '9-206,C', '7-740,D', '1774,B', '7-93,C', '8-97,D', '9-813,B', '9-686,B', '9-799,C', '1179,C', '1954,A', '8-403,D', '9-576,A', '9-866,B', '7-208,C', '9-771,D', '998,C', '433,D', '9-508,A', '7-561,B', '7-976,B', '1635,B', '7-875,A', '7-1053,B', '9-957,C', '1150,C', '8-240,A', '9-554,C', '9-135,B', '7-1096,C', '841,C', '7-146,A', '1554,B', '9-731,C', '1780,C', '7-1077,D', '8-494,D', '936,A', '8-478,B', '9-669,C', '7-732,A', '7-658,A', '1003,B', '8-62,A', '7-386,B', '257,C', '147,B', '7-599,C', '8-92,C', '354,A', '9-966,A', '9-612,C', '9-548,D', '9-429,A', '7-95,C', '1560,A', '9-461,C', '9-490,C', '9-301,C', '60,C', '9-894,C', '9-895,D', '9-281,A', '202,C', '1937,C', '620,C', '8-142,C', '7-1138,A', '8-471,B', '9-433,A', '1458,C', '57,B', '605,C', '9-889,D', '1890,A', '9-618,A', '9-523,A', '1126,D', '644,B', '8-365,A', '9-727,D', '7-461,A', '9-1071,D', '1918,B', '1038,D', '9-197,D', '1393,B', '7-244,B', '9-916,B', '9-1046,A', '167,B', '9-566,B', '8-28,D', '7-179,C', '389,B', '1528,C', '1457,D', '1208,C', '1170,C', '8-409,A', '8-307,D', '1948,D', '661,C', '7-435,C', '8-332,C', '948,B', '381,D', '9-759,B', '8-350,D', '7-727,C', '850,B', '970,D', '7-381,D', '9-436,C', '9-411,C', '9-692,D', '1334,A', '9-1160,D', '9-89,A', '9-1034,D', '8-293,B', '9-652,C', '1391,C', '9-948,B', '8-213,C', '162,A', '1359,B', '9-743,A', '9-645,A', '8-250,C', '283,B', '8-183,A', '9-284,A', '7-1186,C', '926,C', '7-519,B', '7-7,C']\nhere1\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": " 84%|████████▍ | 53/63 [00:39<00:07,  1.33it/s]\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m tr_loss, nb_tr_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m best_dev_acc, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_eval_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_choice_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_max_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                          \u001b[49m\u001b[43margs_max_len_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_external_sent_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_dev_acc:\u001b[39m\u001b[38;5;124m'\u001b[39m,best_dev_acc)\n\u001b[1;32m     67\u001b[0m best_test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[13], line 97\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, test_examples, tokenizer, eval_batch_size, choice_num, max_len, max_len_gen, external_sent_num)\u001b[0m\n\u001b[1;32m     92\u001b[0m batch_example \u001b[38;5;241m=\u001b[39m [example \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m test_examples[beg_index:end_index]]\n\u001b[1;32m     93\u001b[0m q_ids, q_mask, qo_ids, qo_mask, clue_ids, clue_content_ids, clue_content_mask, answers, output_clue \u001b[38;5;241m=\u001b[39m get_input_feature(batch_example,\n\u001b[1;32m     94\u001b[0m                                                                                    max_len, max_len_gen,\n\u001b[1;32m     95\u001b[0m                                                                                    args_choice_num,\n\u001b[1;32m     96\u001b[0m                                                                                    external_sent_num)\n\u001b[0;32m---> 97\u001b[0m scores, output_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqo_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqo_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoice_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    100\u001b[0m answers \u001b[38;5;241m=\u001b[39m answers\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mtolist()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[11], line 58\u001b[0m, in \u001b[0;36mGenMC.forward\u001b[0;34m(self, q_ids, q_mask, qo_ids, qo_mask, choice_num, clue_ids, clue_content_ids, clue_content_mask, answers)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m*\u001b[39m loss_ans\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     opt_score, output_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_option_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqo_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqo_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_score, output_sequences\n",
            "Cell \u001b[0;32mIn[11], line 66\u001b[0m, in \u001b[0;36mGenMC.get_option_score\u001b[0;34m(self, q_ids, q_mask, qo_ids, qo_mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m t5_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt5_model\u001b[38;5;241m.\u001b[39mencoder(input_ids\u001b[38;5;241m=\u001b[39mqo_ids\u001b[38;5;241m.\u001b[39mto(local_device), attention_mask\u001b[38;5;241m=\u001b[39mqo_mask\u001b[38;5;241m.\u001b[39mto(local_device))\n\u001b[1;32m     64\u001b[0m encoder_qo \u001b[38;5;241m=\u001b[39m t5_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m t5_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt5_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m encoder_q \u001b[38;5;241m=\u001b[39m t5_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     68\u001b[0m local_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt5_model\u001b[38;5;241m.\u001b[39mdevice\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1115\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1101\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1102\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         output_attentions,\n\u001b[1;32m   1113\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    705\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    600\u001b[0m ):\n\u001b[0;32m--> 601\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    603\u001b[0m         normed_hidden_states,\n\u001b[1;32m    604\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:255\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 255\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16]:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQk_WqP62_Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = plt.plot(x, y_dev, label = 'training loss')\n",
        "l2 = plt.plot(x, y_train, label = 'validation loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-08T19:30:54.95885Z",
          "iopub.status.idle": "2024-03-08T19:30:54.959226Z",
          "shell.execute_reply.started": "2024-03-08T19:30:54.959047Z",
          "shell.execute_reply": "2024-03-08T19:30:54.959063Z"
        },
        "trusted": true,
        "id": "YH94-shH2_Iv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}